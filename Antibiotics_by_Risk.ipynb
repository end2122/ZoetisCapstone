{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "path_master = r'C:/Users/endwy/Documents/Columbia MSBA/Spring 2019/E4524 - Analytics in Practice/Data/Csv/'\n",
    "file_names = list()\n",
    "for path, subdirs, files in os.walk(path_master):\n",
    "    for filename in files:\n",
    "        file_names.append(filename)\n",
    "df_final = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "with open(path_master+'April 2017 - Antibiotic by Risk2.csv') as fp:\n",
    "    df = pd.read_csv(fp)\n",
    "df.rename(columns={'0':'Drug', '1':'Treated','2':'Repulls', '3':'RepullsB', '4':'Mortalities',\n",
    "                            '5':'Treatment Success(%)', '6':'Case Fatality Rate(%)', \n",
    "                            '8':'$/Hd/Rx','10':'Treatment $','11':'Treatment $B'}, inplace=True)\n",
    "df.fillna(0,inplace=True)\n",
    "merge_cols(df,'Treatment $','Treatment $B')\n",
    "merge_cols(df,'Repulls','RepullsB')\n",
    "# df['Repulls'] = df['Repulls'].map(lambda x: str(x).split('.')[0]) + df['RepullsB']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Functions to read/clean CSVs -- produces df_ABR.csv final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(x):\n",
    "    x = str(x)\n",
    "    x = x.strip().replace(',','')\n",
    "    x = x.replace(' ','')\n",
    "    if re.search(pattern = r'\\w+',string=x):\n",
    "        return x\n",
    "    if x=='-' or x=='0-':\n",
    "        return 0\n",
    "#     if x[0]=='$':\n",
    "#         return float(x[1:].replace(',','').replace('-','0'))\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def add_risk(df,fn):\n",
    "    \"\"\"Add Risk column based on file number\"\"\"  \n",
    "    if fn=='1':\n",
    "            df.drop(df.index[0],inplace=True)\n",
    "            df['Risk']='Low'\n",
    "    if fn=='2':\n",
    "            df['Risk']='Low'\n",
    "    if fn=='3':\n",
    "            df.drop(df.index[0],inplace=True)\n",
    "            df['Risk']='Moderate'\n",
    "    if fn=='4':\n",
    "            df['Risk']='Moderate'\n",
    "    if fn=='5':\n",
    "            df.drop(df.index[0],inplace=True)\n",
    "            df['Risk']='High'\n",
    "    if fn=='6':\n",
    "            df['Risk']='High'\n",
    "    if fn=='7':\n",
    "            df.drop(df.index[0],inplace=True)\n",
    "            df['Risk']='Ultra High'\n",
    "    if fn=='8':\n",
    "            df['Risk']='Ultra High'\n",
    "    if fn=='9':\n",
    "            df.drop(df.index[0],inplace=True)\n",
    "            df['Risk']='Natural'\n",
    "    if fn=='10':\n",
    "            df['Risk']='Natural'\n",
    "            df.drop(df.index[-1],inplace=True)\n",
    "    df.fillna(0,inplace=True)\n",
    "        \n",
    "    \n",
    "def add_season(month):\n",
    "    springs = ['February','March']\n",
    "    falls = ['September','October','November','December']\n",
    "    if month in springs:\n",
    "        season = \"Spring\"\n",
    "    elif month in falls:\n",
    "        season = \"Fall\"\n",
    "    else: \n",
    "        season = \"Other\"\n",
    "    return season\n",
    "\n",
    "def merge_cols(df,col1,col2):\n",
    "    \"\"\"Merge any two columns in df\"\"\"\n",
    "    df[col1] = df[col1].map(lambda x: str(x).split('.')[0]) + df[col2]\n",
    "    del df[col2]\n",
    "    \n",
    "def add_time(df_ABR1):\n",
    "    \"\"\"Add Month, Year, Season columns based on read-in file name\"\"\"\n",
    "    mon=re.search(r'\\w+', file_name)\n",
    "    df_ABR1['Month']= file_name[mon.start():mon.end()]\n",
    "    y=re.search(r'\\d+', file_name)\n",
    "    df_ABR1['Year'] = file_name[y.start():y.end()]            \n",
    "    add_season(file_name[mon.start():mon.end()])\n",
    "    \n",
    "def type1(pm,tt,fn,d='.csv'):\n",
    "    \"\"\"10 columns, missing 7,9, merge 3,4\"\"\"\n",
    "    with open(pm+tt+fn+d) as fp:\n",
    "        df_ABR1 = pd.read_csv(fp, \n",
    "                              usecols=['0','1','2','3','4','5','6','8','10'],\n",
    "                              keep_default_na=False)\n",
    "    df_ABR1.rename(columns={'0':'Drug', '1':'Treated', '2':'Repulls', '3':'Mortalities', '4':'MortalitiesB', \n",
    "                        '5':'Treatment Success(%)', '6':'Case Fatality Rate(%)', '8':'$/Hd/Rx','10':'Treatment $'}, inplace=True)\n",
    "    df_ABR1.fillna(0,inplace=True)\n",
    "    # Concatenate mortalities column\n",
    "    merge_cols(df_ABR1,'Mortalities','MortalitiesB')\n",
    "    add_time(df_ABR1)\n",
    "    add_risk(df_ABR1,fn)\n",
    "    global df_final\n",
    "    df_final = pd.concat([df_final,df_ABR1])\n",
    "\n",
    "def type2(pm,tt,fn,d='.csv'):\n",
    "    \"\"\"9 columns, missing 6,8\"\"\"\n",
    "    with open(pm+tt+fn+d) as fp:\n",
    "        df_ABR1 = pd.read_csv(fp,\n",
    "                              usecols=['0','1','2','3','4','5','7','9'], keep_default_na=False)\n",
    "    df_ABR1.rename(columns={'0':'Drug', '1':'Treated', '2':'Repulls', '3':'Mortalities', '4':'Treatment Success(%)', \n",
    "                            '5':'Case Fatality Rate(%)', '7':'$/Hd/Rx', '9':'Treatment $',}, \n",
    "                       inplace=True)\n",
    "    df_ABR1.fillna(0,inplace=True)\n",
    "    add_time(df_ABR1)\n",
    "    add_risk(df_ABR1,fn)\n",
    "    global df_final\n",
    "    df_final = pd.concat([df_final,df_ABR1])\n",
    "\n",
    "def type3(pm,tt,fn,d='.csv'):\n",
    "    \"\"\"base case\"\"\"\n",
    "    with open(pm+tt+fn+d) as fp:\n",
    "        df_ABR1 = pd.read_csv(fp,\n",
    "                          skiprows=[0], \n",
    "                          usecols=['Unnamed: 1', 'Treated', 'Repulls', 'Mortalities', 'Treatment Success',\n",
    "                                   'Case Fatality Rate','$/Hd/Rx', 'Treatment $'])\n",
    "    df_ABR1.rename(columns={'Unnamed: 1': 'Drug', 'Treatment Success':'Treatment Success(%)',\n",
    "                        'Case Fatality Rate':'Case Fatality Rate(%)'}, inplace=True)\n",
    "    df_ABR1.fillna(0,inplace=True)\n",
    "    add_time(df_ABR1)\n",
    "    add_risk(df_ABR1,fn)\n",
    "    global df_final\n",
    "    df_final = pd.concat([df_final,df_ABR1])\n",
    "\n",
    "def type4(pm,tt,fn,d='.csv'):\n",
    "    \"\"\"11 columns, missing 7,9, merge 2,3\"\"\"\n",
    "    with open(pm+tt+fn+d) as fp:\n",
    "                df_ABR1 = pd.read_csv(fp,\n",
    "                         usecols=['0','1','2','3','4','5','6','8','10','11'],\n",
    "                         keep_default_na=False)\n",
    "    df_ABR1.rename(columns={'0':'Drug', '1':'Treated', '2':'Repulls', '3':'RepullsB', '4':'Mortalities', \n",
    "                        '5':'Treatment Success(%)', '6':'Case Fatality Rate(%)', '8':'$/Hd/Rx',\n",
    "                        '10':'Treatment $','11':'Treatment $B'}, inplace=True)\n",
    "    df_ABR1.fillna(0,inplace=True)\n",
    "    # Concatenate Treatment $ column and Repulls column\n",
    "    merge_cols(df_ABR1,'Treatment $','Treatment $B')\n",
    "    merge_cols(df_ABR1,'Repulls','RepullsB')\n",
    "    add_time(df_ABR1)\n",
    "    add_risk(df_ABR1,fn)\n",
    "    global df_final\n",
    "    df_final = pd.concat([df_final,df_ABR1])\n",
    "\n",
    "def type5(pm,tt,fn,d='.csv'):\n",
    "    \"\"\"10 column, missing 6,8, merge 9,10\"\"\"\n",
    "    with open(pm+tt+fn+d) as fp:\n",
    "                df_ABR1 = pd.read_csv(fp,\n",
    "                         usecols=['0','1','2','3','4','5','7','9','10'],\n",
    "                         keep_default_na=False)\n",
    "    df_ABR1.rename(columns={'0':'Drug', '1':'Treated', '2':'Repulls', '3':'Mortalities', '4':'Treatment Success(%)', \n",
    "                        '5':'Case Fatality Rate(%)', '7':'$/Hd/Rx', \n",
    "                        '9':'Treatment $','10':'Treatment $B'}, inplace=True)\n",
    "    df_ABR1.fillna(0,inplace=True)\n",
    "    # Concatenate Treatment $ column\n",
    "    merge_cols(df_ABR1,'Treatment $','Treatment $B')\n",
    "    add_time(df_ABR1)\n",
    "    add_risk(df_ABR1,fn)\n",
    "    global df_final\n",
    "    df_final = pd.concat([df_final,df_ABR1])\n",
    "\n",
    "def type6(pm,tt,fn,d='.csv'):\n",
    "    \"\"\"11 column, missing 2, merge 4,5 and 8,9 and 10,11\"\"\"\n",
    "    with open(path_master+title+file_num+'.csv') as fp:\n",
    "                df_ABR1 = pd.read_csv(fp,\n",
    "                         usecols=['0','1','3','4','5','6','7','8','9','10','11'],\n",
    "                         keep_default_na=False)\n",
    "    df_ABR1.rename(columns={'0':'Drug', '1':'Treated', '3':'Repulls', '4':'Mortalities','5':'MortalitiesB','6':'Treatment Success(%)', \n",
    "                        '7':'Case Fatality Rate(%)', '8':'$/Hd/Rx', '9':'$/Hd/RxB',\n",
    "                        '10':'Treatment $','11':'Treatment $B'}, inplace=True)\n",
    "    df_ABR1.fillna(0,inplace=True)\n",
    "    # Concatenate Treatment $ column\n",
    "    merge_cols(df_ABR1,'Mortalities','MortalitiesB')\n",
    "    merge_cols(df_ABR1,'$/Hd/Rx','$/Hd/RxB')\n",
    "    merge_cols(df_ABR1,'Treatment $','Treatment $B')\n",
    "    df_ABR1['Treated']=df_ABR1['Treated'].apply(lambda x: x.replace(' ',''))\n",
    "    add_time(df_ABR1)\n",
    "    add_risk(df_ABR1,fn)\n",
    "    global df_final\n",
    "    df_final = pd.concat([df_final,df_ABR1])\n",
    "\n",
    "def type7(pm,tt,fn,d='.csv'):\n",
    "    \"\"\"11 columns, missing 8,10\"\"\"\n",
    "    with open(path_master+title+file_num+'.csv') as fp:\n",
    "                df_ABR1 = pd.read_csv(fp,\n",
    "                         usecols=['0','1','2','4','6','7','9','11'], keep_default_na=False)\n",
    "    df_ABR1.rename(columns={'0':'Drug', '1':'Treated', '2':'Repulls', '4':'Mortalities', '6':'Treatment Success(%)', \n",
    "                        '7':'Case Fatality Rate(%)', '9':'$/Hd/Rx', '11':'Treatment $',\n",
    "                        }, inplace=True)\n",
    "    df_ABR1.fillna(0,inplace=True)\n",
    "    add_time(df_ABR1)\n",
    "    add_risk(df_ABR1,fn)\n",
    "    global df_final\n",
    "    df_final = pd.concat([df_final,df_ABR1])\n",
    "\n",
    "def type8(pm,tt,fn,d='.csv'):\n",
    "    \"\"\"10 columns, missing 8,9, concatenate Mortalities column\"\"\"\n",
    "    with open(pm+tt+fn+d) as fp:\n",
    "        df_ABR1 = pd.read_csv(fp, \n",
    "                              usecols=['0','1','2','3','4','5','6','7','10'],\n",
    "                              keep_default_na=False)\n",
    "    df_ABR1.rename(columns={'0':'Drug', '1':'Treated', '2':'Repulls', '3':'Mortalities', '4':'MortalitiesB', \n",
    "                        '5':'Treatment Success(%)', '6':'Case Fatality Rate(%)', '7':'$/Hd/Rx','10':'Treatment $'},\n",
    "                   inplace=True)\n",
    "    df_ABR1.fillna(0,inplace=True)\n",
    "    # Concatenate mortalities column\n",
    "    merge_cols(df_ABR1,'Mortalities','MortalitiesB')\n",
    "    add_time(df_ABR1)\n",
    "    add_risk(df_ABR1,fn)\n",
    "    global df_final\n",
    "    df_final = pd.concat([df_final,df_ABR1])\n",
    "\n",
    "def type9(pm,tt,fn,d='.csv'):\n",
    "    \"\"\"13 columns, missing 10,12\"\"\"\n",
    "    with open(pm+tt+fn+d) as fp:\n",
    "        df_ABR1 = pd.read_csv(fp, \n",
    "                              usecols=['0','1','2','3','5','6','8','9','11','13'],\n",
    "                              keep_default_na=False)\n",
    "    df_ABR1.rename(columns={'0':'Drug', '1':'Treated','2':'Repulls','3':'RepullsB','5':'Mortalities','6':'MortalitiesB',\n",
    "                            '8':'Treatment Success(%)','9':'Case Fatality Rate(%)','11':'$/Hd/Rx','13':'Treatment $'},\n",
    "                   inplace=True)\n",
    "    df_ABR1.fillna(0,inplace=True)\n",
    "    # Concatenate Repulls & Mortalities columns\n",
    "    merge_cols(df_ABR1,'Mortalities','MortalitiesB')\n",
    "    merge_cols(df_ABR1,'Repulls','RepullsB')\n",
    "    add_time(df_ABR1)\n",
    "    add_risk(df_ABR1,fn)\n",
    "    global df_final\n",
    "    df_final = pd.concat([df_final,df_ABR1])\n",
    "\n",
    "def type10(pm,tt,fn,d='.csv'):\n",
    "    \"\"\"9 columns, missing 8, merge 3,4\"\"\"\n",
    "    with open(pm+tt+fn+d) as fp:\n",
    "        df_ABR1 = pd.read_csv(fp, \n",
    "                              usecols=['0','1','2','3','4','5','6','7','9'],\n",
    "                              keep_default_na=False)\n",
    "    df_ABR1.rename(columns={'0':'Drug', '1':'Treated', '2':'Repulls', '3':'Mortalities', '4':'MortalitiesB', \n",
    "                        '5':'Treatment Success(%)', '6':'Case Fatality Rate(%)', '7':'$/Hd/Rx','9':'Treatment $'}, \n",
    "                   inplace=True)\n",
    "    df_ABR1.fillna(0,inplace=True)\n",
    "    # Concatenate mortalities column\n",
    "    merge_cols(df_ABR1,'Mortalities','MortalitiesB')\n",
    "    add_time(df_ABR1)\n",
    "    add_risk(df_ABR1,fn)\n",
    "    global df_final\n",
    "    df_final = pd.concat([df_final,df_ABR1])\n",
    "    \n",
    "def type11(pm,tt,fn,d='.csv'):\n",
    "    \"\"\"11 columns, missing 3,8,10 merge 4,5\"\"\"\n",
    "    with open(path_master+title+file_num+'.csv') as fp:\n",
    "                df_ABR1 = pd.read_csv(fp,\n",
    "                         usecols=['0','1','2','4','5','6','7','9','11'], keep_default_na=False)\n",
    "    df_ABR1.rename(columns={'0':'Drug', '1':'Treated', '2':'Repulls', '4':'Mortalities', '5':'MortalitiesB',\n",
    "                            '6':'Treatment Success(%)', '7':'Case Fatality Rate(%)', '9':'$/Hd/Rx', '11':'Treatment $',\n",
    "                        }, inplace=True)\n",
    "    df_ABR1.fillna(0,inplace=True)\n",
    "    # Concatenate mortalities column\n",
    "    merge_cols(df_ABR1,'Mortalities','MortalitiesB')\n",
    "    add_time(df_ABR1)\n",
    "    add_risk(df_ABR1,fn)\n",
    "    global df_final\n",
    "    df_final = pd.concat([df_final,df_ABR1])\n",
    "    \n",
    "def type12(pm,tt,fn,d='.csv'):\n",
    "    \"\"\"11 columns, missing 8,10, merge 2,3 and 4,5\"\"\"\n",
    "    with open(path_master+title+file_num+'.csv') as fp:\n",
    "                df_ABR1 = pd.read_csv(fp,\n",
    "                         usecols=['0','1','2','3','4','5','6','7','9','11'], keep_default_na=False)\n",
    "    df_ABR1.rename(columns={'0':'Drug', '1':'Treated', '2':'Repulls', '3':'RepullsB','4':'Mortalities','5':'MortalitiesB',\n",
    "                            '6':'Treatment Success(%)','7':'Case Fatality Rate(%)', '9':'$/Hd/Rx', '11':'Treatment $',\n",
    "                        }, inplace=True)\n",
    "    df_ABR1.fillna(0,inplace=True)\n",
    "    # Concatenate Repulls & Mortalities columns\n",
    "    merge_cols(df_ABR1,'Mortalities','MortalitiesB')\n",
    "    merge_cols(df_ABR1,'Repulls','RepullsB')\n",
    "    add_time(df_ABR1)\n",
    "    add_risk(df_ABR1,fn)\n",
    "    global df_final\n",
    "    df_final = pd.concat([df_final,df_ABR1])\n",
    "    \n",
    "def type13(pm,tt,fn,d='.csv'):\n",
    "    \"\"\"11 column, missing 7,9, merge 2,3 and 10,11\"\"\"\n",
    "    with open(path_master+title+file_num+'.csv') as fp:\n",
    "                df_ABR1 = pd.read_csv(fp,\n",
    "                         usecols=['0','1','2','3','4','5','6','8','10','11'],\n",
    "                         keep_default_na=False)\n",
    "    df_ABR1.rename(columns={'0':'Drug', '1':'Treated','2':'Repulls', '3':'RepullsB', '4':'Mortalities',\n",
    "                            '5':'Treatment Success(%)', '6':'Case Fatality Rate(%)', \n",
    "                            '8':'$/Hd/Rx','10':'Treatment $','11':'Treatment $B'}, inplace=True)\n",
    "    df_ABR1.fillna(0,inplace=True)\n",
    "    # Concatenate Treatment $ column\n",
    "    merge_cols(df_ABR1,'Repulls','RepullsB')\n",
    "    merge_cols(df_ABR1,'Treatment $','Treatment $B')\n",
    "    df_ABR1['Treated']=df_ABR1['Treated'].apply(lambda x: x.replace(' ',''))\n",
    "    add_time(df_ABR1)\n",
    "    add_risk(df_ABR1,fn)\n",
    "    global df_final\n",
    "    df_final = pd.concat([df_final,df_ABR1])\n",
    "    \n",
    "def type14(pm,tt,fn,d='.csv'):\n",
    "    \"\"\"9 columns, missing 7,8\"\"\"\n",
    "    with open(pm+tt+fn+d) as fp:\n",
    "        df_ABR1 = pd.read_csv(fp,\n",
    "                              usecols=['0','1','2','3','4','5','6','9'], keep_default_na=False)\n",
    "    df_ABR1.rename(columns={'0':'Drug', '1':'Treated', '2':'Repulls', '3':'Mortalities', '4':'Treatment Success(%)', \n",
    "                            '5':'Case Fatality Rate(%)', '6':'$/Hd/Rx', '9':'Treatment $',}, \n",
    "                       inplace=True)\n",
    "    df_ABR1.fillna(0,inplace=True)\n",
    "    add_time(df_ABR1)\n",
    "    add_risk(df_ABR1,fn)\n",
    "    global df_final\n",
    "    df_final = pd.concat([df_final,df_ABR1])\n",
    "    \n",
    "def type15(pm,tt,fn,d='.csv'):\n",
    "    \"\"\"10 columns, missing 2,7,9\"\"\"\n",
    "    with open(pm+tt+fn+d) as fp:\n",
    "        df_ABR1 = pd.read_csv(fp, \n",
    "                              usecols=['0','1','3','4','5','6','8','10'],\n",
    "                              keep_default_na=False)\n",
    "    df_ABR1.rename(columns={'0':'Drug', '1':'Treated', '3':'Repulls','4':'Mortalities','5':'Treatment Success(%)', \n",
    "                            '6':'Case Fatality Rate(%)', '8':'$/Hd/Rx','10':'Treatment $'}, inplace=True)\n",
    "    df_ABR1.fillna(0,inplace=True)\n",
    "    # Concatenate mortalities column\n",
    "    add_time(df_ABR1)\n",
    "    add_risk(df_ABR1,fn)\n",
    "    global df_final\n",
    "    df_final = pd.concat([df_final,df_ABR1])\n",
    "    \n",
    "def type16(pm,tt,fn,d='.csv'):\n",
    "    \"\"\"11 columns, missing 2,4,8,10\"\"\"\n",
    "    with open(path_master+title+file_num+'.csv') as fp:\n",
    "                df_ABR1 = pd.read_csv(fp,\n",
    "                         usecols=['0','1','3','5','6','7','9','11'], keep_default_na=False)\n",
    "    df_ABR1.rename(columns={'0':'Drug', '1':'Treated', '3':'Repulls', '5':'Mortalities', '6':'Treatment Success(%)', \n",
    "                        '7':'Case Fatality Rate(%)', '9':'$/Hd/Rx', '11':'Treatment $',\n",
    "                        }, inplace=True)\n",
    "    df_ABR1.fillna(0,inplace=True)\n",
    "    add_time(df_ABR1)\n",
    "    add_risk(df_ABR1,fn)\n",
    "    global df_final\n",
    "    df_final = pd.concat([df_final,df_ABR1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use functions to clean each month's CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Usecols do not match columns, columns expected but not found: ['11']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-976aa9fc43c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    103\u001b[0m                 \u001b[0mtype3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_master\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfile_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mfile_num\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'2'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m                 \u001b[0mtype13\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_master\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfile_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mfile_num\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'3'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfile_num\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'4'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfile_num\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'6'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfile_num\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'10'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m                 \u001b[0mtype1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_master\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfile_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-a45454319d72>\u001b[0m in \u001b[0;36mtype13\u001b[1;34m(pm, tt, fn, d)\u001b[0m\n\u001b[0;32m    271\u001b[0m                 df_ABR1 = pd.read_csv(fp,\n\u001b[0;32m    272\u001b[0m                          \u001b[0musecols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'0'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'3'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'4'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'5'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'6'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'8'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'10'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'11'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m                          keep_default_na=False)\n\u001b[0m\u001b[0;32m    274\u001b[0m     df_ABR1.rename(columns={'0':'Drug', '1':'Treated','2':'Repulls', '3':'RepullsB', '4':'Mortalities',\n\u001b[0;32m    275\u001b[0m                             \u001b[1;34m'5'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'Treatment Success(%)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'6'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'Case Fatality Rate(%)'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1747\u001b[0m             if (self.usecols_dtype == 'string' and\n\u001b[0;32m   1748\u001b[0m                     not set(usecols).issubset(self.orig_names)):\n\u001b[1;32m-> 1749\u001b[1;33m                 \u001b[0m_validate_usecols_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morig_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1751\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_validate_usecols_names\u001b[1;34m(usecols, names)\u001b[0m\n\u001b[0;32m   1132\u001b[0m         raise ValueError(\n\u001b[0;32m   1133\u001b[0m             \u001b[1;34m\"Usecols do not match columns, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1134\u001b[1;33m             \u001b[1;34m\"columns expected but not found: {missing}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmissing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1135\u001b[0m         )\n\u001b[0;32m   1136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Usecols do not match columns, columns expected but not found: ['11']"
     ]
    }
   ],
   "source": [
    "# JANUARY 2017 & 2018 (note: no 2017 file)\n",
    "for file_name in file_names:\n",
    "    pattern = r'January\\s+\\d+\\s+\\-\\s+Antibiotic by Risk'\n",
    "    title = file_name.split(\"Risk\")[0]+str(\"Risk\")\n",
    "    if re.search(pattern=pattern, string=file_name):\n",
    "        file_num=re.findall(r'\\d+', file_name)[-1]\n",
    "        # cleaning for 2018\n",
    "        if file_name.split()[1] == '2018':\n",
    "            if file_num == '1':\n",
    "                type3(path_master,title,file_num)\n",
    "            elif file_num == '2':\n",
    "                type4(path_master,title,file_num)\n",
    "            elif file_num == '3' or file_num == '5' or file_num == '8'or file_num == '10':\n",
    "                type1(path_master,title,file_num)\n",
    "            elif file_num == '4':\n",
    "                type5(path_master,title,file_num)\n",
    "            elif file_num == '6' or file_num == '7'or file_num == '9':\n",
    "                type2(path_master,title,file_num)    \n",
    "\n",
    "# FEBRUARY 2017 & 2018\n",
    "for file_name in file_names:\n",
    "    pattern = r'February\\s+\\d+\\s+\\-\\s+Antibiotic by Risk'\n",
    "    title = file_name.split(\"Risk\")[0]+str(\"Risk\")\n",
    "    if re.search(pattern=pattern, string=file_name):\n",
    "        file_num=re.findall(r'\\d+', file_name)[-1]\n",
    "        # cleaning for 2017\n",
    "        if file_name.split()[1] == '2017':\n",
    "            if file_num == '1':\n",
    "                type3(path_master,title,file_num)\n",
    "            elif file_num == '2':\n",
    "                type4(path_master,title,file_num)\n",
    "            elif file_num == '3' or file_num == '5'or file_num == '8':\n",
    "                type1(path_master,title,file_num)\n",
    "            elif file_num == '4'or file_num == '9':\n",
    "                type2(path_master,title,file_num)\n",
    "            elif file_num == '6':\n",
    "                type5(path_master,title,file_num)\n",
    "            elif file_num == '7'or file_num == '10':\n",
    "                type7(path_master,title,file_num)\n",
    "        # cleaning for 2018\n",
    "        elif file_name.split()[1] == '2018':\n",
    "            if file_num == '1':\n",
    "                type3(path_master,title,file_num)\n",
    "            elif file_num == '2':\n",
    "                type4(path_master,title,file_num)\n",
    "            elif file_num == '3' or file_num == '10':\n",
    "                type1(path_master,title,file_num)\n",
    "            elif file_num == '4':\n",
    "                type5(path_master,title,file_num)\n",
    "            elif file_num == '5':\n",
    "                type8(path_master,title,file_num)\n",
    "            elif file_num == '6' or file_num == '7' or file_num == '9':\n",
    "                type2(path_master,title,file_num)\n",
    "            elif file_num == '8':\n",
    "                type7(path_master,title,file_num)\n",
    "        \n",
    "# MARCH 2017 & 2018 (note: no 2017 file)\n",
    "for file_name in file_names:\n",
    "    pattern = r'March\\s+\\d+\\s+\\-\\s+Antibiotic by Risk'\n",
    "    title = file_name.split(\"Risk\")[0]+str(\"Risk\")\n",
    "    if re.search(pattern=pattern, string=file_name):\n",
    "        file_num=re.findall(r'\\d+', file_name)[-1]\n",
    "        # cleaning for 2018\n",
    "        if file_name.split()[1] == '2018':\n",
    "            if file_num == '1':\n",
    "                type3(path_master,title,file_num)\n",
    "            elif file_num == '2':\n",
    "                type4(path_master,title,file_num)\n",
    "            elif file_num == '3':\n",
    "                type7(path_master,title,file_num)\n",
    "            elif file_num == '4' or file_num == '5'or file_num == '6'or file_num == '7'or file_num == '8':\n",
    "                type1(path_master,title,file_num)\n",
    "            elif file_num == '9':\n",
    "                type2(path_master,title,file_num)\n",
    "            elif file_num == '10':\n",
    "                type9(path_master,title,file_num)\n",
    "    \n",
    "# APRIL 2017 & 2018\n",
    "for file_name in file_names:\n",
    "    pattern = r'April\\s+\\d+\\s+\\-\\s+Antibiotic by Risk'\n",
    "    title = file_name.split(\"Risk\")[0]+str(\"Risk\")\n",
    "    if re.search(pattern=pattern, string=file_name):\n",
    "        file_num=re.findall(r'\\d+', file_name)[-1]\n",
    "        # cleaning for 2017\n",
    "        if file_name.split()[1] == '2017':\n",
    "            if file_num == '3' or file_num == '5' or file_num == '8':\n",
    "                type1(path_master,title,file_num)\n",
    "            elif file_num == '4' or file_num == '9':\n",
    "                type2(path_master,title,file_num)\n",
    "            elif file_num == '1':\n",
    "                type3(path_master,title,file_num)\n",
    "            elif file_num == '2':\n",
    "                type13(path_master,title,file_num)\n",
    "            elif file_num == '6':\n",
    "                type5(path_master,title,file_num)\n",
    "            elif file_num == '7':\n",
    "                type6(path_master,title,file_num)\n",
    "            elif file_num == '10':\n",
    "                type7(path_master,title,file_num)\n",
    "        # cleaning for 2018\n",
    "        elif file_name.split()[1] == '2018':\n",
    "            if file_num == '1':\n",
    "                type3(path_master,title,file_num)\n",
    "            elif file_num == '2':\n",
    "                type13(path_master,title,file_num)\n",
    "            elif file_num == '3' or file_num=='4' or file_num=='6' or file_num=='10':\n",
    "                type1(path_master,title,file_num)  \n",
    "            elif file_num=='5' or file_num=='9':\n",
    "                type2(path_master,title,file_num)\n",
    "            elif file_num == '7':\n",
    "                type15(path_master,title,file_num)\n",
    "            elif file_num == '8':\n",
    "                type1(path_master,title,file_num)\n",
    "                \n",
    "# MAY 2017 & 2018 (note: no 2017 file)\n",
    "for file_name in file_names:\n",
    "    pattern = r'May\\s+\\d+\\s+\\-\\s+Antibiotic by Risk'\n",
    "    title = file_name.split(\"Risk\")[0]+str(\"Risk\")\n",
    "    if re.search(pattern=pattern, string=file_name):\n",
    "        file_num=re.findall(r'\\d+', file_name)[-1]\n",
    "        # cleaning for 2018\n",
    "        if file_name.split()[1] == '2018':\n",
    "            if file_num == '1':\n",
    "                type3(path_master,title,file_num)\n",
    "            elif file_num == '2':\n",
    "                type5(path_master,title,file_num)\n",
    "            elif file_num == '3' or file_num == '10':\n",
    "                type1(path_master,title,file_num)\n",
    "            elif file_num == '4' or file_num == '5' or file_num == '6'or file_num == '7'or file_num == '8'or file_num == '9':\n",
    "                type2(path_master,title,file_num)\n",
    "                \n",
    "# JUNE 2017 & 2018 (note: no 2017 file)\n",
    "for file_name in file_names:\n",
    "    pattern = r'June\\s+\\d+\\s+\\-\\s+Antibiotic by Risk'\n",
    "    title = file_name.split(\"Risk\")[0]+str(\"Risk\")\n",
    "    if re.search(pattern=pattern, string=file_name):\n",
    "        file_num=re.findall(r'\\d+', file_name)[-1]\n",
    "        # cleaning for 2018\n",
    "        if file_name.split()[1] == '2018':\n",
    "            if file_num == '1':\n",
    "                type3(path_master,title,file_num)\n",
    "            elif file_num == '2':\n",
    "                type7(path_master,title,file_num)\n",
    "            elif file_num == '3' or file_num == '4' or file_num == '10':\n",
    "                type1(path_master,title,file_num)\n",
    "            elif file_num == '5' or file_num == '6' or file_num == '7'or file_num == '8':\n",
    "                type2(path_master,title,file_num)\n",
    "            elif file_num == '9':\n",
    "                type5(path_master,title,file_num)\n",
    "                \n",
    "# JULY 2017 & 2018 (note: no 2017 file)\n",
    "for file_name in file_names:\n",
    "    pattern = r'July\\s+\\d+\\s+\\-\\s+Antibiotic by Risk'\n",
    "    title = file_name.split(\"Risk\")[0]+str(\"Risk\")\n",
    "    if re.search(pattern=pattern, string=file_name):\n",
    "        file_num=re.findall(r'\\d+', file_name)[-1]\n",
    "        # cleaning for 2018\n",
    "        if file_name.split()[1] == '2018':\n",
    "            if file_num == '1':\n",
    "                type3(path_master,title,file_num)\n",
    "            elif file_num == '2' or file_num == '4' or file_num == '5' or file_num == '6' or file_num == '7' or file_num == '8':\n",
    "                type2(path_master,title,file_num)\n",
    "            elif file_num == '3':\n",
    "                type1(path_master,title,file_num)\n",
    "            elif file_num == '9':\n",
    "                type5(path_master,title,file_num)\n",
    "            elif file_num == '10':\n",
    "                type12(path_master,title,file_num)\n",
    "                \n",
    "# AUGUST 2017 & 2018\n",
    "for file_name in file_names:\n",
    "    pattern = r'August\\s+\\d+\\s+\\-\\s+Antibiotic by Risk'\n",
    "    title = file_name.split(\"Risk\")[0]+str(\"Risk\")\n",
    "    if re.search(pattern=pattern, string=file_name):\n",
    "        file_num=re.findall(r'\\d+', file_name)[-1]\n",
    "        # cleaning for 2017\n",
    "        if file_name.split()[1] == '2017':\n",
    "            if file_num == '1':\n",
    "                type3(path_master,title,file_num)\n",
    "            elif file_num == '2' or file_num == '4' or file_num == '7' or file_num == '8':\n",
    "                type2(path_master,title,file_num)\n",
    "            elif file_num == '3' or file_num == '5':\n",
    "                type1(path_master,title,file_num)\n",
    "            elif file_num == '6':\n",
    "                type5(path_master,title,file_num)\n",
    "            elif file_num == '9':\n",
    "                type4(path_master,title,file_num)\n",
    "            elif file_num == '10':\n",
    "                type7(path_master,title,file_num)\n",
    "        # cleaning for 2018\n",
    "        if file_name.split()[1] == '2018':\n",
    "            if file_num == '1':\n",
    "                type3(path_master,title,file_num)\n",
    "            elif file_num == '2' or file_num == '4' or file_num == '5' or file_num == '7' or file_num == '9':\n",
    "                type2(path_master,title,file_num)\n",
    "            elif file_num == '3':\n",
    "                type10(path_master,title,file_num)\n",
    "            elif file_num == '6' or file_num == '8':\n",
    "                type1(path_master,title,file_num)\n",
    "            elif file_num == '10':\n",
    "                type12(path_master,title,file_num)\n",
    "                \n",
    "# SEPTEMBER 2017 & 2018 (note: no 2017 file)\n",
    "for file_name in file_names:\n",
    "    pattern = r'September\\s+\\d+\\s+\\-\\s+Antibiotic by Risk'\n",
    "    title = file_name.split(\"Risk\")[0]+str(\"Risk\")\n",
    "    if re.search(pattern=pattern, string=file_name):\n",
    "        file_num=re.findall(r'\\d+', file_name)[-1]\n",
    "        # cleaning for 2018\n",
    "        if file_name.split()[1] == '2018':\n",
    "            if file_num == '1':\n",
    "                type3(path_master,title,file_num)\n",
    "            elif file_num == '2' or file_num == '3' or file_num == '5' or file_num == '8' or file_num == '10':\n",
    "                type1(path_master,title,file_num)\n",
    "            elif file_num == '4':\n",
    "                type5(path_master,title,file_num)\n",
    "            elif file_num == '6' or file_num == '7' or file_num == '9':\n",
    "                type2(path_master,title,file_num)\n",
    "                \n",
    "# OCTOBER 2017 & 2018 (note: no 2017 file)\n",
    "for file_name in file_names:\n",
    "    pattern = r'October\\s+\\d+\\s+\\-\\s+Antibiotic by Risk'\n",
    "    title = file_name.split(\"Risk\")[0]+str(\"Risk\")\n",
    "    if re.search(pattern=pattern, string=file_name):\n",
    "        file_num=re.findall(r'\\d+', file_name)[-1]\n",
    "        # cleaning for 2018\n",
    "        if file_name.split()[1] == '2018':\n",
    "            if file_num == '1':\n",
    "                type3(path_master,title,file_num)\n",
    "            elif file_num == '2':\n",
    "                type12(path_master,title,file_num)\n",
    "            elif file_num == '3' or file_num == '8' or file_num == '10':\n",
    "                type1(path_master,title,file_num)\n",
    "            elif file_num == '4':\n",
    "                type13(path_master,title,file_num)\n",
    "            elif file_num == '5':\n",
    "                type14(path_master,title,file_num)\n",
    "            elif file_num == '6' or file_num == '7' or file_num == '9':\n",
    "                type2(path_master,title,file_num)\n",
    "            \n",
    "# NOVEMBER 2017 & 2018 (note: no 2017 or 2018 file)\n",
    "\n",
    "# DECEMBER 2017 & 2018 (note: no 2018 file)\n",
    "for file_name in file_names:\n",
    "    pattern = r'December\\s+\\d+\\s+\\-\\s+Antibiotic by Risk'\n",
    "    title = file_name.split(\"Risk\")[0]+str(\"Risk\")\n",
    "    if re.search(pattern=pattern, string=file_name):\n",
    "        file_num=re.findall(r'\\d+', file_name)[-1]\n",
    "        # cleaning for 2017\n",
    "        if file_name.split()[1] == '2017':\n",
    "            if file_num == '1':\n",
    "                type3(path_master,title,file_num)\n",
    "            elif file_num == '2' or file_num == '4':\n",
    "                type5(path_master,title,file_num)\n",
    "            elif file_num == '3' or file_num == '4' or file_num == '8' or file_num == '10':\n",
    "                type1(path_master,title,file_num)\n",
    "            elif file_num == '6' or file_num == '7' or file_num == '9':\n",
    "                type2(path_master,title,file_num)\n",
    "\n",
    "# Remove percent signs from columns and convery str -> float\n",
    "df_final['Treatment Success(%)'] = df_final['Treatment Success(%)'].apply(lambda x: float(x[:-1]))\n",
    "df_final['Case Fatality Rate(%)'] = df_final['Case Fatality Rate(%)'].apply(lambda x: float(x[:-1]))\n",
    "\n",
    "# add season column\n",
    "df_final['Season'] = df_final['Month'].apply(add_season)\n",
    "\n",
    "# Remove dollar signs from columns\n",
    "df_final['Treatment $'] = df_final['Treatment $'].apply(lambda x: x[1:])\n",
    "df_final['$/Hd/Rx'] = df_final['$/Hd/Rx'].apply(lambda x: x[1:])\n",
    "\n",
    "#clean number formats\n",
    "for col in df_final.columns:\n",
    "    df_final[col] = df_final[col].apply(conv)\n",
    "\n",
    "# dtypes to numeric\n",
    "col_to_num = ['Treated','Repulls','Mortalities','Treatment $','$/Hd/Rx']\n",
    "for i in col_to_num:\n",
    "    df_final[i] = df_final[i].convert_objects(convert_numeric=True)\n",
    "\n",
    "# fill zeroes\n",
    "df_final.fillna(0)\n",
    "\n",
    "# Save out df_ABR\n",
    "import csv\n",
    "df_final.to_csv(path_master+\"/df_ABR.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open full df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "with open(path_master+'df_ABR.csv') as fp:\n",
    "    df_final = pd.read_csv(fp)\n",
    "del df_final['Unnamed: 0']\n",
    "df_final.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
